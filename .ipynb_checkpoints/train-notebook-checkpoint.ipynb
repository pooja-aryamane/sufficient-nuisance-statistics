{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random \n",
    "import torch \n",
    "import os \n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "from torch import nn\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import image\n",
    "import shutil\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix, balanced_accuracy_score\n",
    "\n",
    "from dataloader import * \n",
    "from models import * \n",
    "from train import * \n",
    "from train_gdro import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 6, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpooja-aryamane\u001b[0m (\u001b[33msuff-nuisance-stats\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.16.6 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/scratch/paa9751/mlhc-project/wandb/run-20240409_151232-avz3hpcl\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdrawn-monkey-6\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/suff-nuisance-stats/stage-1-training-sample-split\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/suff-nuisance-stats/stage-1-training-sample-split/runs/avz3hpcl\u001b[0m\n",
      "starting training!\n",
      "training started\n",
      "/ext3/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 6, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "107.45243602991104\n",
      "1.9536806550892916\n",
      "train loss = 0, group 0 -  0.06738139926032587\n",
      "train loss = 0, group 1 -  3.5518870028582485\n",
      "train loss = 0, group 2 -  3.886679909446023\n",
      "train loss = 0, group 3 -  0.05236723001369021\n",
      "val loop\n",
      "/ext3/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 6, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "27.875776529312134\n",
      "2.322981377442678\n",
      "/ext3/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 6, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/site-packages/IPython/utils/_process_posix.py:153\u001b[0m, in \u001b[0;36mProcessHandler.system\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;66;03m# res is the index of the pattern that caused the match, so we\u001b[39;00m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;66;03m# know whether we've finished (if we matched EOF) or not\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m     res_idx \u001b[38;5;241m=\u001b[39m child\u001b[38;5;241m.\u001b[39mexpect_list(patterns, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_timeout)\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28mprint\u001b[39m(child\u001b[38;5;241m.\u001b[39mbefore[out_size:]\u001b[38;5;241m.\u001b[39mdecode(enc, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreplace\u001b[39m\u001b[38;5;124m'\u001b[39m), end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/site-packages/pexpect/spawnbase.py:372\u001b[0m, in \u001b[0;36mSpawnBase.expect_list\u001b[0;34m(self, pattern_list, timeout, searchwindowsize, async_, **kw)\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 372\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m exp\u001b[38;5;241m.\u001b[39mexpect_loop(timeout)\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/site-packages/pexpect/expect.py:169\u001b[0m, in \u001b[0;36mExpecter.expect_loop\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;66;03m# Still have time left, so read more data\u001b[39;00m\n\u001b[0;32m--> 169\u001b[0m incoming \u001b[38;5;241m=\u001b[39m spawn\u001b[38;5;241m.\u001b[39mread_nonblocking(spawn\u001b[38;5;241m.\u001b[39mmaxread, timeout)\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspawn\u001b[38;5;241m.\u001b[39mdelayafterread \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/site-packages/pexpect/pty_spawn.py:500\u001b[0m, in \u001b[0;36mspawn.read_nonblocking\u001b[0;34m(self, size, timeout)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;66;03m# Because of the select(0) check above, we know that no data\u001b[39;00m\n\u001b[1;32m    498\u001b[0m \u001b[38;5;66;03m# is available right now. But if a non-zero timeout is given\u001b[39;00m\n\u001b[1;32m    499\u001b[0m \u001b[38;5;66;03m# (possibly timeout=None), we call select() with a timeout.\u001b[39;00m\n\u001b[0;32m--> 500\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (timeout \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m select(timeout):\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(spawn, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mread_nonblocking(size)\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/site-packages/pexpect/pty_spawn.py:450\u001b[0m, in \u001b[0;36mspawn.read_nonblocking.<locals>.select\u001b[0;34m(timeout)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect\u001b[39m(timeout):\n\u001b[0;32m--> 450\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m select_ignore_interrupts([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchild_fd], [], [], timeout)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/site-packages/pexpect/utils.py:143\u001b[0m, in \u001b[0;36mselect_ignore_interrupts\u001b[0;34m(iwtd, owtd, ewtd, timeout)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m select\u001b[38;5;241m.\u001b[39mselect(iwtd, owtd, ewtd, timeout)\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython3 train.py --PROJECT_NAME \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstage-1-training-sample-split\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m --BORDER_SZ 25 --LEARNING_RATE 1e-4 --BATCH_SIZE 128 --WEIGHT_DECAY 1e-3 --MODEL_TYPE \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresnet18\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m --OUTDIR out-nuisance-samplesplit --MAX_EPOCHS 200 --SPLIT_IDX 0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/site-packages/ipykernel/zmqshell.py:655\u001b[0m, in \u001b[0;36mZMQInteractiveShell.system_piped\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    653\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_ns[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_exit_code\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m system(cmd)\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 655\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_ns[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_exit_code\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m system(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvar_expand(cmd, depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/site-packages/IPython/utils/_process_posix.py:164\u001b[0m, in \u001b[0;36mProcessHandler.system\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    159\u001b[0m         out_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(child\u001b[38;5;241m.\u001b[39mbefore)\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;66;03m# We need to send ^C to the process.  The ascii code for '^C' is 3\u001b[39;00m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# (the character is known as ETX for 'End of Text', see\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# curses.ascii.ETX).\u001b[39;00m\n\u001b[0;32m--> 164\u001b[0m     child\u001b[38;5;241m.\u001b[39msendline(\u001b[38;5;28mchr\u001b[39m(\u001b[38;5;241m3\u001b[39m))\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# Read and print any more output the program might produce on its\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;66;03m# way out.\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/site-packages/pexpect/pty_spawn.py:578\u001b[0m, in \u001b[0;36mspawn.sendline\u001b[0;34m(self, s)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''Wraps send(), sending string ``s`` to child process, with\u001b[39;00m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;124;03m``os.linesep`` automatically appended. Returns number of bytes\u001b[39;00m\n\u001b[1;32m    574\u001b[0m \u001b[38;5;124;03mwritten.  Only a limited number of bytes may be sent for each\u001b[39;00m\n\u001b[1;32m    575\u001b[0m \u001b[38;5;124;03mline in the default terminal mode, see docstring of :meth:`send`.\u001b[39;00m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    577\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_coerce_send_string(s)\n\u001b[0;32m--> 578\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(s \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinesep)\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/site-packages/pexpect/pty_spawn.py:563\u001b[0m, in \u001b[0;36mspawn.send\u001b[0;34m(self, s)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''Sends string ``s`` to the child process, returning the number of\u001b[39;00m\n\u001b[1;32m    529\u001b[0m \u001b[38;5;124;03mbytes written. If a logfile is specified, a copy is written to that\u001b[39;00m\n\u001b[1;32m    530\u001b[0m \u001b[38;5;124;03mlog.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;124;03m    >>> bash.sendline('x' * 5000)\u001b[39;00m\n\u001b[1;32m    560\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelaybeforesend \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 563\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelaybeforesend)\n\u001b[1;32m    565\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_coerce_send_string(s)\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log(s, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msend\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "!python3 train.py --PROJECT_NAME 'stage-1-training-sample-split' --BORDER_SZ 25 --LEARNING_RATE 1e-4 --BATCH_SIZE 128 --WEIGHT_DECAY 1e-3 --MODEL_TYPE 'resnet18' --OUTDIR out-nuisance-samplesplit --MAX_EPOCHS 200 --SPLIT_IDX 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_LABEL='negbio'\n",
    "IMAGE_SIZE=256\n",
    "TRANSFORM=False\n",
    "NWORKERS=6\n",
    "BATCH_SIZE=12\n",
    "BORDER_SZ=0\n",
    "K=10000\n",
    "MAX_EPOCHS=100\n",
    "LEARNING_RATE=1e-3\n",
    "WEIGHT_DECAY=0\n",
    "LOGFILE=\"./logs-ntbk\"\n",
    "OUTDIR=\"./out-ntbk\"\n",
    "MODEL_TYPE=\"resnet18\"\n",
    "PRETRAINED=False\n",
    "RESIZE=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no resize-mimic\n",
      "no resize-mimic\n",
      "no resize-mimic\n",
      "no resize-chx\n",
      "no resize-chx\n",
      "no resize-chx\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# logging.basicConfig(filename='/scratch/paa9751/mlhc-project/' + LOGFILE + \"/\" + str(BORDER_SZ) + str(LEARNING_RATE) + str(BATCH_SIZE)+'train.log',filemode='a',level=logging.DEBUG)\n",
    "\n",
    "# logging.info(\"********************** START **********************\")\n",
    "\n",
    "if MODEL_TYPE == 'resnet18':\n",
    "    logging.info(\"using resnet18\")\n",
    "    model = Resnet18(num_classes=1, pretrained=PRETRAINED)\n",
    "elif MODEL_TYPE =='resnet50':\n",
    "    logging.info(\"using resnet50\")\n",
    "    model = Resnet50(num_classes=1, pretrained=PRETRAINED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device) \n",
    "# logging.info(f'device : {device}')\n",
    "\n",
    "chexpert_dir = '/scratch/paa9751/mlhc-project/resized_data/chexpert'\n",
    "mimic_dir = '/scratch/paa9751/mlhc-project/resized_data/mimic'\n",
    "\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "#     train_dataloader, val_dataloader, test_dataloader = load_combined_data(chexpert_dir, (mimic_dir, M_LABEL), IMAGE_SIZE, TRANSFORM, NWORKERS, BATCH_SIZE, BORDER_SZ, K)  \n",
    "\n",
    "train_dataloader, val_dataloader, test_dataloader = load_resized_data(chexpert_dir, mimic_dir, IMAGE_SIZE, TRANSFORM, NWORKERS, BATCH_SIZE, RESIZE, BORDER_SZ) \n",
    "\n",
    "print(PRETRAINED)\n",
    "print(TRANSFORM)\n",
    "print(RESIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000\n",
      "1500\n",
      "1500\n"
     ]
    }
   ],
   "source": [
    "chexpert_dir = '/scratch/paa9751/mlhc-project/resized_data/chexpert'\n",
    "mimic_dir = '/scratch/paa9751/mlhc-project/resized_data/mimic'\n",
    "\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "#     train_dataloader, val_dataloader, test_dataloader = load_combined_data(chexpert_dir, (mimic_dir, M_LABEL), IMAGE_SIZE, TRANSFORM, NWORKERS, BATCH_SIZE, BORDER_SZ, K)  \n",
    "\n",
    "NORMALISE=True\n",
    "\n",
    "train_dataloader, val_dataloader, test_dataloader = load_resized_data(chexpert_dir, mimic_dir, IMAGE_SIZE, NORMALISE, TRANSFORM, NWORKERS, BATCH_SIZE, BORDER_SZ, 'true_group_idx', 0) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training started\n",
      "traindone\n",
      "Epoch 1 : Robust loss = 0.6260863906083806, Robust Val loss = 0.7298113961219788, Val Acc = 0.6856666666666666, Val AUROC = 0.751386284828186, Val F1 = 0.7339915633201599\n",
      "traindone\n",
      "Epoch 2 : Robust loss = 0.5307965531511091, Robust Val loss = 1.3914094527959824, Val Acc = 0.538, Val AUROC = 0.8422902822494507, Val F1 = 0.16304348409175873\n",
      "traindone\n",
      "Epoch 3 : Robust loss = 0.3432371012911188, Robust Val loss = 0.34631349295377734, Val Acc = 0.5, Val AUROC = 0.38403555750846863, Val F1 = 0.6666666865348816\n",
      "traindone\n",
      "Epoch 4 : Robust loss = 0.32713639266284594, Robust Val loss = 0.3216891015172005, Val Acc = 0.5, Val AUROC = 0.4296689033508301, Val F1 = 0.6666666865348816\n",
      "traindone\n",
      "Epoch 5 : Robust loss = 0.32917288707724063, Robust Val loss = 0.33827142739295957, Val Acc = 0.5, Val AUROC = 0.46442151069641113, Val F1 = 0.6666666865348816\n",
      "traindone\n",
      "Epoch 6 : Robust loss = 0.332472544747637, Robust Val loss = 0.3214006513655186, Val Acc = 0.5, Val AUROC = 0.3284653425216675, Val F1 = 0.6666666865348816\n",
      "traindone\n",
      "Epoch 7 : Robust loss = 0.33001119842642684, Robust Val loss = 0.3350698517858982, Val Acc = 0.5, Val AUROC = 0.5169051289558411, Val F1 = 0.6666666865348816\n",
      "traindone\n",
      "Epoch 8 : Robust loss = 0.329863538622422, Robust Val loss = 0.32511973291635515, Val Acc = 0.5, Val AUROC = 0.3674629032611847, Val F1 = 0.6666666865348816\n",
      "traindone\n",
      "Epoch 9 : Robust loss = 0.33224490150528374, Robust Val loss = 0.32033245819807055, Val Acc = 0.5, Val AUROC = 0.3172740042209625, Val F1 = 0.6666666865348816\n",
      "traindone\n",
      "Epoch 10 : Robust loss = 0.3289524837562235, Robust Val loss = 0.3252386707961559, Val Acc = 0.5, Val AUROC = 0.4011177718639374, Val F1 = 0.6666666865348816\n",
      "training works\n",
      "traindone\n",
      "Epoch 11 : Robust loss = 0.33195185121835147, Robust Val loss = 0.3297268761098385, Val Acc = 0.5, Val AUROC = 0.3697870969772339, Val F1 = 0.6666666865348816\n",
      "traindone\n",
      "Epoch 12 : Robust loss = 0.32744246299898105, Robust Val loss = 0.3300249074399471, Val Acc = 0.5, Val AUROC = 0.5619044899940491, Val F1 = 0.6666666865348816\n",
      "traindone\n",
      "Epoch 13 : Robust loss = 0.32683433121790106, Robust Val loss = 0.3359695595800877, Val Acc = 0.5, Val AUROC = 0.3766506612300873, Val F1 = 0.6666666865348816\n",
      "traindone\n",
      "Epoch 14 : Robust loss = 0.3283738430235104, Robust Val loss = 0.3316017817258835, Val Acc = 0.5, Val AUROC = 0.5325520634651184, Val F1 = 0.6666666865348816\n",
      "traindone\n",
      "Epoch 15 : Robust loss = 0.3246388428181333, Robust Val loss = 0.3211309752464294, Val Acc = 0.5, Val AUROC = 0.3568171262741089, Val F1 = 0.6666666865348816\n",
      "traindone\n",
      "Epoch 16 : Robust loss = 0.3264050804199882, Robust Val loss = 0.33151078006625173, Val Acc = 0.5, Val AUROC = 0.33782491087913513, Val F1 = 0.6666666865348816\n",
      "traindone\n",
      "Epoch 17 : Robust loss = 0.3295529369414874, Robust Val loss = 0.32168782415986064, Val Acc = 0.5, Val AUROC = 0.34624356031417847, Val F1 = 0.6666666865348816\n",
      "traindone\n",
      "Epoch 18 : Robust loss = 0.3308054534167741, Robust Val loss = 0.32013649052381515, Val Acc = 0.5, Val AUROC = 0.3191269040107727, Val F1 = 0.6666666865348816\n",
      "traindone\n",
      "Epoch 19 : Robust loss = 0.32618232220475146, Robust Val loss = 0.32093024533987047, Val Acc = 0.5, Val AUROC = 0.32126399874687195, Val F1 = 0.6666666865348816\n",
      "traindone\n",
      "Epoch 20 : Robust loss = 0.3296357730998887, Robust Val loss = 0.32247513115406035, Val Acc = 0.5, Val AUROC = 0.3466891050338745, Val F1 = 0.6666666865348816\n",
      "training works\n",
      "traindone\n",
      "Epoch 21 : Robust loss = 0.32327905077017977, Robust Val loss = 0.3368345477581024, Val Acc = 0.5, Val AUROC = 0.36103641986846924, Val F1 = 0.6666666865348816\n",
      "traindone\n",
      "Epoch 22 : Robust loss = 0.3246896472131481, Robust Val loss = 0.32615609183907507, Val Acc = 0.5, Val AUROC = 0.35667353868484497, Val F1 = 0.6666666865348816\n",
      "traindone\n",
      "Epoch 23 : Robust loss = 0.32728438512857083, Robust Val loss = 0.3318331038057804, Val Acc = 0.5, Val AUROC = 0.40605130791664124, Val F1 = 0.6666666865348816\n",
      "traindone\n",
      "Epoch 24 : Robust loss = 0.3227483074866305, Robust Val loss = 0.33210570868849754, Val Acc = 0.5, Val AUROC = 0.3757755756378174, Val F1 = 0.6666666865348816\n",
      "traindone\n",
      "Epoch 25 : Robust loss = 0.33126834072587624, Robust Val loss = 0.3318537773191929, Val Acc = 0.5, Val AUROC = 0.36392664909362793, Val F1 = 0.6666666865348816\n",
      "traindone\n",
      "Epoch 26 : Robust loss = 0.3232636680633417, Robust Val loss = 0.32662262612581255, Val Acc = 0.5, Val AUROC = 0.4233658015727997, Val F1 = 0.6666666865348816\n",
      "traindone\n",
      "Epoch 27 : Robust loss = 0.3238326788631738, Robust Val loss = 0.31974205404520034, Val Acc = 0.5, Val AUROC = 0.3863697648048401, Val F1 = 0.6666666865348816\n",
      "traindone\n",
      "Epoch 28 : Robust loss = 0.3272223515242926, Robust Val loss = 0.32500654822587965, Val Acc = 0.5, Val AUROC = 0.36464816331863403, Val F1 = 0.6666666865348816\n",
      "traindone\n",
      "Epoch 29 : Robust loss = 0.31992380836565165, Robust Val loss = 0.33048197759687903, Val Acc = 0.5, Val AUROC = 0.4192661941051483, Val F1 = 0.6666666865348816\n",
      "traindone\n",
      "Epoch 30 : Robust loss = 0.3236583271179581, Robust Val loss = 0.32168831062316894, Val Acc = 0.5, Val AUROC = 0.3978453278541565, Val F1 = 0.6666666865348816\n",
      "training works\n",
      "traindone\n",
      "Epoch 31 : Robust loss = 0.32576790479456347, Robust Val loss = 0.32424767130613324, Val Acc = 0.5, Val AUROC = 0.40039312839508057, Val F1 = 0.6666666865348816\n",
      "traindone\n",
      "Epoch 32 : Robust loss = 0.3196282149663621, Robust Val loss = 0.32563570219278337, Val Acc = 0.5, Val AUROC = 0.43921756744384766, Val F1 = 0.6666666865348816\n",
      "traindone\n",
      "Epoch 33 : Robust loss = 0.31436599561531214, Robust Val loss = 0.34449805414676665, Val Acc = 0.5, Val AUROC = 0.4405255615711212, Val F1 = 0.6666666865348816\n",
      "traindone\n",
      "Epoch 34 : Robust loss = 0.3180526109203224, Robust Val loss = 0.3278682822287083, Val Acc = 0.5, Val AUROC = 0.379425585269928, Val F1 = 0.6666666865348816\n",
      "traindone\n",
      "Epoch 35 : Robust loss = 0.3148928515823764, Robust Val loss = 0.3277036291956902, Val Acc = 0.5, Val AUROC = 0.4013144373893738, Val F1 = 0.6666666865348816\n",
      "traindone\n",
      "Epoch 36 : Robust loss = 0.31399869424990845, Robust Val loss = 0.33284337519109247, Val Acc = 0.5, Val AUROC = 0.3794442117214203, Val F1 = 0.6666666865348816\n",
      "traindone\n",
      "Epoch 37 : Robust loss = 0.311147907051324, Robust Val loss = 0.3349272323995829, Val Acc = 0.5, Val AUROC = 0.45131799578666687, Val F1 = 0.6666666865348816\n",
      "traindone\n",
      "Epoch 38 : Robust loss = 0.31565337664197934, Robust Val loss = 0.3399118587374687, Val Acc = 0.5, Val AUROC = 0.43906712532043457, Val F1 = 0.6666666865348816\n",
      "traindone\n",
      "Epoch 39 : Robust loss = 0.3040626611729769, Robust Val loss = 0.3468209444135427, Val Acc = 0.5, Val AUROC = 0.4141184389591217, Val F1 = 0.6666666865348816\n",
      "traindone\n",
      "Epoch 40 : Robust loss = 0.3035886390860916, Robust Val loss = 0.33910791140794755, Val Acc = 0.5, Val AUROC = 0.43468424677848816, Val F1 = 0.6666666865348816\n",
      "training works\n",
      "traindone\n",
      "Epoch 41 : Robust loss = 0.3017643000619309, Robust Val loss = 0.3609914736524224, Val Acc = 0.49966666666666665, Val AUROC = 0.4509275555610657, Val F1 = 0.6663703322410583\n",
      "traindone\n",
      "Epoch 42 : Robust loss = 0.29544143616233814, Robust Val loss = 0.3548015127554536, Val Acc = 0.499, Val AUROC = 0.42077288031578064, Val F1 = 0.6654796600341797\n",
      "traindone\n",
      "Epoch 43 : Robust loss = 0.2934771649644813, Robust Val loss = 0.3556849249266088, Val Acc = 0.5, Val AUROC = 0.42114001512527466, Val F1 = 0.6665184497833252\n",
      "traindone\n",
      "Epoch 44 : Robust loss = 0.28340548276901245, Robust Val loss = 0.37302498249709604, Val Acc = 0.5013333333333333, Val AUROC = 0.4548902213573456, Val F1 = 0.6636690497398376\n",
      "traindone\n",
      "Epoch 45 : Robust loss = 0.27465244130197125, Robust Val loss = 0.3698566623479128, Val Acc = 0.5013333333333333, Val AUROC = 0.428987592458725, Val F1 = 0.6636690497398376\n",
      "traindone\n",
      "Epoch 46 : Robust loss = 0.26239206679700056, Robust Val loss = 0.36552554063498977, Val Acc = 0.5006666666666667, Val AUROC = 0.4311639964580536, Val F1 = 0.6654756665229797\n",
      "traindone\n",
      "Epoch 47 : Robust loss = 0.2519790465826202, Robust Val loss = 0.39296461740136146, Val Acc = 0.501, Val AUROC = 0.44813889265060425, Val F1 = 0.6653252840042114\n",
      "traindone\n",
      "Epoch 48 : Robust loss = 0.23869223554635416, Robust Val loss = 0.39218919393606483, Val Acc = 0.5013333333333333, Val AUROC = 0.44614177942276, Val F1 = 0.6650246381759644\n",
      "traindone\n",
      "Epoch 49 : Robust loss = 0.22761315547708733, Robust Val loss = 0.4024316538516432, Val Acc = 0.5086666666666667, Val AUROC = 0.4861944615840912, Val F1 = 0.6684660315513611\n",
      "traindone\n",
      "Epoch 50 : Robust loss = 0.20259903891861197, Robust Val loss = 0.4606013879030943, Val Acc = 0.499, Val AUROC = 0.4558820128440857, Val F1 = 0.6495686769485474\n",
      "training works\n",
      "traindone\n",
      "Epoch 51 : Robust loss = 0.18646976060111112, Robust Val loss = 0.47909926912561057, Val Acc = 0.49633333333333335, Val AUROC = 0.43993711471557617, Val F1 = 0.6485229134559631\n",
      "traindone\n",
      "Epoch 52 : Robust loss = 0.16362849350132036, Robust Val loss = 0.45853128721006214, Val Acc = 0.5076666666666667, Val AUROC = 0.4749041795730591, Val F1 = 0.6543412208557129\n",
      "traindone\n",
      "Epoch 53 : Robust loss = 0.14637048525241744, Robust Val loss = 0.5619113052829634, Val Acc = 0.5023333333333333, Val AUROC = 0.4735691249370575, Val F1 = 0.659365713596344\n",
      "traindone\n",
      "Epoch 54 : Robust loss = 0.14218834735778932, Robust Val loss = 0.5855807585869915, Val Acc = 0.5086666666666667, Val AUROC = 0.4675599932670593, Val F1 = 0.6584800481796265\n",
      "traindone\n",
      "Epoch 55 : Robust loss = 0.11297511675607252, Robust Val loss = 0.5936672919932753, Val Acc = 0.5033333333333333, Val AUROC = 0.47336578369140625, Val F1 = 0.6596619486808777\n",
      "traindone\n",
      "Epoch 56 : Robust loss = 0.12283520969903049, Robust Val loss = 0.6173457015743042, Val Acc = 0.5033333333333333, Val AUROC = 0.46066421270370483, Val F1 = 0.6541318297386169\n",
      "traindone\n",
      "Epoch 57 : Robust loss = 0.1012546924847314, Robust Val loss = 0.5778125863456517, Val Acc = 0.5103333333333333, Val AUROC = 0.4790613353252411, Val F1 = 0.6600323915481567\n",
      "traindone\n",
      "Epoch 58 : Robust loss = 0.08504589302202338, Robust Val loss = 0.7601928851905977, Val Acc = 0.492, Val AUROC = 0.4128977656364441, Val F1 = 0.6477115154266357\n",
      "traindone\n",
      "Epoch 59 : Robust loss = 0.07965898318123213, Robust Val loss = 0.6777193649513429, Val Acc = 0.504, Val AUROC = 0.45500776171684265, Val F1 = 0.6515222191810608\n",
      "traindone\n",
      "Epoch 60 : Robust loss = 0.06422674987060627, Robust Val loss = 0.8733916198540246, Val Acc = 0.5046666666666667, Val AUROC = 0.4944055378437042, Val F1 = 0.6568129062652588\n",
      "training works\n",
      "traindone\n",
      "Epoch 61 : Robust loss = 0.07629475113830324, Robust Val loss = 0.7786386915490712, Val Acc = 0.5016666666666667, Val AUROC = 0.4576628804206848, Val F1 = 0.6515963673591614\n",
      "traindone\n",
      "Epoch 62 : Robust loss = 0.06559425268788771, Robust Val loss = 0.8280810014563612, Val Acc = 0.49366666666666664, Val AUROC = 0.45159468054771423, Val F1 = 0.6431759595870972\n",
      "traindone\n",
      "Epoch 63 : Robust loss = 0.05628646903444754, Robust Val loss = 0.8775166502170905, Val Acc = 0.5036666666666667, Val AUROC = 0.4968964457511902, Val F1 = 0.6445452570915222\n",
      "traindone\n",
      "Epoch 64 : Robust loss = 0.05325272140776263, Robust Val loss = 0.8361044544976175, Val Acc = 0.5016666666666667, Val AUROC = 0.45686018466949463, Val F1 = 0.6560846567153931\n",
      "traindone\n",
      "Epoch 65 : Robust loss = 0.04496015399182408, Robust Val loss = 1.0468115126424309, Val Acc = 0.49233333333333335, Val AUROC = 0.46143490076065063, Val F1 = 0.6284459829330444\n",
      "traindone\n",
      "Epoch 66 : Robust loss = 0.042684928252875835, Robust Val loss = 0.8596521616802311, Val Acc = 0.49666666666666665, Val AUROC = 0.4464802145957947, Val F1 = 0.6515920758247375\n",
      "traindone\n",
      "Epoch 67 : Robust loss = 0.044289442662464984, Robust Val loss = 1.0173360523326664, Val Acc = 0.5013333333333333, Val AUROC = 0.45223087072372437, Val F1 = 0.6507936716079712\n",
      "traindone\n",
      "Epoch 68 : Robust loss = 0.03678503198719118, Robust Val loss = 0.9915119066503103, Val Acc = 0.49333333333333335, Val AUROC = 0.4671764373779297, Val F1 = 0.6426892280578613\n",
      "traindone\n",
      "Epoch 69 : Robust loss = 0.046581656312522014, Robust Val loss = 0.8536893783669538, Val Acc = 0.5033333333333333, Val AUROC = 0.4704626798629761, Val F1 = 0.6531657576560974\n",
      "traindone\n",
      "Epoch 70 : Robust loss = 0.03594029518707449, Robust Val loss = 0.9911812177951045, Val Acc = 0.5066666666666667, Val AUROC = 0.4844008982181549, Val F1 = 0.660861611366272\n",
      "training works\n",
      "traindone\n",
      "Epoch 71 : Robust loss = 0.036590289049954317, Robust Val loss = 0.9911793236334877, Val Acc = 0.5116666666666667, Val AUROC = 0.5021606683731079, Val F1 = 0.6590644717216492\n",
      "traindone\n",
      "Epoch 72 : Robust loss = 0.033787661904763906, Robust Val loss = 1.0389428766717101, Val Acc = 0.49933333333333335, Val AUROC = 0.46638110280036926, Val F1 = 0.650860071182251\n",
      "traindone\n",
      "Epoch 73 : Robust loss = 0.03428327355226338, Robust Val loss = 1.143527268541834, Val Acc = 0.5043333333333333, Val AUROC = 0.4565422236919403, Val F1 = 0.662582278251648\n",
      "traindone\n",
      "Epoch 74 : Robust loss = 0.04032202830074171, Robust Val loss = 0.8859231121976336, Val Acc = 0.504, Val AUROC = 0.4493580162525177, Val F1 = 0.6573007702827454\n",
      "traindone\n",
      "Epoch 75 : Robust loss = 0.029653415214160955, Robust Val loss = 1.0959834286148835, Val Acc = 0.5063333333333333, Val AUROC = 0.48988646268844604, Val F1 = 0.6170157790184021\n",
      "traindone\n",
      "Epoch 76 : Robust loss = 0.026145294545772427, Robust Val loss = 1.1110532747117319, Val Acc = 0.5026666666666667, Val AUROC = 0.4635186791419983, Val F1 = 0.6560627222061157\n",
      "traindone\n",
      "Epoch 77 : Robust loss = 0.028566633938821095, Robust Val loss = 1.1251830918382064, Val Acc = 0.5023333333333333, Val AUROC = 0.45946067571640015, Val F1 = 0.6598314046859741\n",
      "traindone\n",
      "Epoch 78 : Robust loss = 0.023662838651628538, Robust Val loss = 1.0330869382939973, Val Acc = 0.5046666666666667, Val AUROC = 0.4766000211238861, Val F1 = 0.658233642578125\n",
      "traindone\n",
      "Epoch 79 : Robust loss = 0.02880366107144204, Robust Val loss = 1.1938999378129174, Val Acc = 0.5196666666666667, Val AUROC = 0.5263633728027344, Val F1 = 0.6550155878067017\n",
      "traindone\n",
      "Epoch 80 : Robust loss = 0.031462590324570154, Robust Val loss = 0.9631920920299298, Val Acc = 0.5073333333333333, Val AUROC = 0.45346131920814514, Val F1 = 0.6505910158157349\n",
      "training works\n",
      "traindone\n",
      "Epoch 81 : Robust loss = 0.01975285887433565, Robust Val loss = 1.154833856015709, Val Acc = 0.5093333333333333, Val AUROC = 0.4810713529586792, Val F1 = 0.6637734174728394\n",
      "traindone\n",
      "Epoch 82 : Robust loss = 0.026328992540375046, Robust Val loss = 1.0880674004253452, Val Acc = 0.5056666666666667, Val AUROC = 0.4897106885910034, Val F1 = 0.6369644999504089\n",
      "traindone\n",
      "Epoch 83 : Robust loss = 0.024048117450786832, Robust Val loss = 1.0638026212472396, Val Acc = 0.5103333333333333, Val AUROC = 0.4974926710128784, Val F1 = 0.6592438220977783\n",
      "traindone\n",
      "Epoch 84 : Robust loss = 0.02160212515344171, Robust Val loss = 1.1739631968480957, Val Acc = 0.49966666666666665, Val AUROC = 0.46701645851135254, Val F1 = 0.6573841571807861\n",
      "traindone\n",
      "Epoch 85 : Robust loss = 0.014052255957259471, Robust Val loss = 1.306441411524668, Val Acc = 0.49866666666666665, Val AUROC = 0.4728095531463623, Val F1 = 0.6508820652961731\n",
      "traindone\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x14553255dc60>\n",
      "Traceback (most recent call last):\n",
      "  File \"/ext3/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1478, in __del__\n",
      "Exception in thread Thread-299 (_pin_memory_loop):\n",
      "Traceback (most recent call last):\n",
      "  File \"/ext3/miniconda3/lib/python3.11/threading.py\", line 1038, in _bootstrap_inner\n",
      "    self._shutdown_workers()\n",
      "  File \"/ext3/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1424, in _shutdown_workers\n",
      "    self._pin_memory_thread.join()\n",
      "  File \"/ext3/miniconda3/lib/python3.11/threading.py\", line 1112, in join\n",
      "    self.run()\n",
      "  File \"/ext3/miniconda3/lib/python3.11/threading.py\", line 975, in run\n",
      "    self._wait_for_tstate_lock()\n",
      "  File \"/ext3/miniconda3/lib/python3.11/threading.py\", line 1132, in _wait_for_tstate_lock\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/ext3/miniconda3/lib/python3.11/site-packages/torch/utils/data/_utils/pin_memory.py\", line 54, in _pin_memory_loop\n",
      "    if lock.acquire(block, timeout):\n",
      "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt: \n",
      "    do_one_step()\n",
      "  File \"/ext3/miniconda3/lib/python3.11/site-packages/torch/utils/data/_utils/pin_memory.py\", line 31, in do_one_step\n",
      "    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/ext3/miniconda3/lib/python3.11/multiprocessing/queues.py\", line 122, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/ext3/miniconda3/lib/python3.11/site-packages/torch/multiprocessing/reductions.py\", line 355, in rebuild_storage_fd\n",
      "    fd = df.detach()\n",
      "         ^^^^^^^^^^^\n",
      "  File \"/ext3/miniconda3/lib/python3.11/multiprocessing/resource_sharer.py\", line 57, in detach\n",
      "    with _resource_sharer.get_connection(self._id) as conn:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/ext3/miniconda3/lib/python3.11/multiprocessing/resource_sharer.py\", line 86, in get_connection\n",
      "    c = Client(address, authkey=process.current_process().authkey)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/ext3/miniconda3/lib/python3.11/multiprocessing/connection.py\", line 507, in Client\n",
      "    answer_challenge(c, authkey)\n",
      "  File \"/ext3/miniconda3/lib/python3.11/multiprocessing/connection.py\", line 756, in answer_challenge\n",
      "    response = connection.recv_bytes(256)        # reject large message\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/ext3/miniconda3/lib/python3.11/multiprocessing/connection.py\", line 215, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/ext3/miniconda3/lib/python3.11/multiprocessing/connection.py\", line 413, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "          ^^^^^^^^^^^^^\n",
      "  File \"/ext3/miniconda3/lib/python3.11/multiprocessing/connection.py\", line 378, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m start_training_robust(model, train_dataloader, val_dataloader, device)\n",
      "Cell \u001b[0;32mIn[29], line 93\u001b[0m, in \u001b[0;36mstart_training_robust\u001b[0;34m(model, train_dataloader, val_dataloader, device)\u001b[0m\n\u001b[1;32m     91\u001b[0m batch_x \u001b[38;5;241m=\u001b[39m batch_x\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     92\u001b[0m batch_y \u001b[38;5;241m=\u001b[39m batch_y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 93\u001b[0m pred \u001b[38;5;241m=\u001b[39m model(batch_x\u001b[38;5;241m.\u001b[39mfloat())\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     94\u001b[0m predicted \u001b[38;5;241m=\u001b[39m (sigmoid(pred)\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m     95\u001b[0m correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (predicted \u001b[38;5;241m==\u001b[39m batch_y)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem() \n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/scratch/paa9751/mlhc-project/models.py:28\u001b[0m, in \u001b[0;36mResnet18.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x): \n\u001b[1;32m     27\u001b[0m     \u001b[38;5;66;03m#x = self.conv_map(x)\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresnet18(x) \n\u001b[1;32m     29\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     30\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc_out(x)\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/site-packages/torchvision/models/resnet.py:96\u001b[0m, in \u001b[0;36mBasicBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     93\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(out)\n\u001b[1;32m     94\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n\u001b[0;32m---> 96\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(out)\n\u001b[1;32m     97\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn2(out)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conv_forward(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1682\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1673\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;241m=\u001b[39m OrderedDict()\n\u001b[1;32m   1675\u001b[0m \u001b[38;5;66;03m# On the return type:\u001b[39;00m\n\u001b[1;32m   1676\u001b[0m \u001b[38;5;66;03m# We choose to return `Any` in the `__getattr__` type signature instead of a more strict `Union[Tensor, Module]`.\u001b[39;00m\n\u001b[1;32m   1677\u001b[0m \u001b[38;5;66;03m# This is done for better interop with various type checkers for the end users.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1680\u001b[0m \u001b[38;5;66;03m# See full discussion on the problems with returning `Union` here\u001b[39;00m\n\u001b[1;32m   1681\u001b[0m \u001b[38;5;66;03m# https://github.com/microsoft/pyright/issues/4213\u001b[39;00m\n\u001b[0;32m-> 1682\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m   1683\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[1;32m   1684\u001b[0m         _parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_training_robust(model, train_dataloader, val_dataloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper functions: \n",
    "def compute_group_avg(losses, group_idx):\n",
    "    # compute observed counts and mean loss for each group\n",
    "    n_groups = torch.unique(group_idx).shape[0]\n",
    "    group_map = (group_idx == torch.arange(n_groups).unsqueeze(1).long()).float()\n",
    "    group_count = group_map.sum(1)\n",
    "    group_denom = group_count + (group_count==0).float() # avoid nans\n",
    "    group_loss = (group_map @ losses.view(-1))/group_denom #check loss dim \n",
    "    return group_loss, group_count\n",
    "\n",
    "def compute_robust_loss(group_probs, step_size, group_loss): \n",
    "    #they either normalise group loss or adjust group loss by adding adj/sqrt(count)\n",
    "    group_probs = group_probs * torch.exp(step_size * group_loss)\n",
    "    group_probs = group_probs/group_probs.sum()\n",
    "    robust_loss = group_loss @ group_probs\n",
    "    return robust_loss, group_probs.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_LABEL='negbio'\n",
    "IMAGE_SIZE=256\n",
    "TRANSFORM=False\n",
    "NWORKERS=6\n",
    "BATCH_SIZE=12\n",
    "BORDER_SZ=0\n",
    "K=10000\n",
    "MAX_EPOCHS=100\n",
    "LEARNING_RATE=1e-3\n",
    "WEIGHT_DECAY=0\n",
    "LOGFILE=\"./logs-ntbk\"\n",
    "OUTDIR=\"./out-ntbk/out-erm\"\n",
    "MODEL_TYPE=\"resnet18\"\n",
    "PRETRAINED=False\n",
    "RESIZE=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_training( model, train_dataloader, val_dataloader, device):\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-2)\n",
    "   # scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "    training_losses=[]\n",
    "    validation_losses=[]\n",
    "    validation_accuracy=[]\n",
    "    #validation_metrics={}\n",
    "\n",
    "    auroc_metric = BinaryAUROC(thresholds=None)\n",
    "    f1_metric = BinaryF1Score()\n",
    "    print(\"training started\")\n",
    "    for epoch in range(MAX_EPOCHS): \n",
    "        #print('training started.')\n",
    "        #train model \n",
    "        model.train()\n",
    "        train_loss=0\n",
    "        for batch_x, batch_y,l in train_dataloader: #16, 100, 512, 512 \n",
    "            batch_x = batch_x.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            \n",
    "            pred = model(batch_x.float())\n",
    "            pred = pred.squeeze(1)\n",
    "            loss = criterion(pred, batch_y.float()) \n",
    "            train_loss+=loss.item()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            #print('iteration done')\n",
    "            \n",
    "        #print(f'epoch {epoch}, train done')\n",
    "\n",
    "        train_loss = train_loss/len(train_dataloader)\n",
    "        training_losses.append(train_loss)\n",
    "\n",
    "        val_loss=0\n",
    "        correct=0\n",
    "        total=0\n",
    "        all_preds=[]\n",
    "        all_labels=[]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            sigmoid = torch.nn.Sigmoid()\n",
    "            for batch_x, batch_y,l in val_dataloader:\n",
    "                batch_x = batch_x.to(device)\n",
    "                batch_y = batch_y.to(device)\n",
    "                pred = model(batch_x.float()).squeeze(1)\n",
    "                predicted = (sigmoid(pred)>0.5).float()\n",
    "                correct += (predicted == batch_y).sum().item() \n",
    "                total+=batch_y.size(0)\n",
    "                loss = criterion(pred, batch_y.float()) \n",
    "                val_loss += loss.item()\n",
    "                all_preds.append(pred.cpu())\n",
    "                all_labels.append(batch_y.cpu())\n",
    "\n",
    "            val_loss = val_loss / len(val_dataloader)\n",
    "            val_acc = correct / total \n",
    "            validation_losses.append(val_loss)\n",
    "            validation_accuracy.append(val_acc)\n",
    "            val_f1 = f1_metric(torch.cat(all_preds,0), torch.cat(all_labels,0))\n",
    "            val_auroc = auroc_metric(torch.cat(all_preds,0), torch.cat(all_labels,0))\n",
    "\n",
    "        #if (epoch+1)%10 == 0: \n",
    "        #logging.info(f\"Epoch {epoch+1} : Training loss = {train_loss}, Val loss = {val_loss}, Val Acc = {val_acc}, Val AUROC = {val_auroc}, Val F1 = {val_f1}\")\n",
    "        print(f\"Epoch {epoch+1} : Training loss = {train_loss}, Val loss = {val_loss}, Val Acc = {val_acc}, Val AUROC = {val_auroc}, Val F1 = {val_f1}\")\n",
    "\n",
    "        if (epoch+1)%10 == 0:\n",
    "            print('training works')\n",
    "            torch.save(model, f'{OUTDIR}/{str(BORDER_SZ) + MODEL_TYPE + str(PRETRAINED) + str(BATCH_SIZE)}_model_{epoch+1}.pth')\n",
    "\n",
    "        #before_lr = optimizer.param_groups[0][\"lr\"]\n",
    "        #scheduler.step()\n",
    "        #after_lr = optimizer.param_groups[0][\"lr\"]\n",
    "        #logging.info(\"Epoch %d: Exponential lr %.4f -> %.4f\" % (epoch, before_lr, after_lr))\n",
    "    \n",
    "            np.save(f'{OUTDIR}/{str(epoch+1)+str(BORDER_SZ) + str(LEARNING_RATE) + str(BATCH_SIZE)}_training_losses.npy', np.array(training_losses))\n",
    "            np.save(f'{OUTDIR}/{str(epoch+1)+str(BORDER_SZ) + str(LEARNING_RATE) + str(BATCH_SIZE)}_validation_losses.npy', np.array(validation_losses))\n",
    "            np.save(f'{OUTDIR}/{str(epoch+1)+str(BORDER_SZ) + str(LEARNING_RATE) + str(BATCH_SIZE)}_validation_accuracy.npy', np.array(validation_accuracy))\n",
    "\n",
    "    torch.save(model, f'{OUTDIR}/final_model.pth')\n",
    "    print('done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training started\n",
      "Epoch 1 : Training loss = 0.7845264417857519, Val loss = 0.597670646905899, Val Acc = 0.68, Val AUROC = 0.743606686592102, Val F1 = 0.6701030731201172\n",
      "Epoch 2 : Training loss = 0.5721321789852656, Val loss = 0.5452308354377746, Val Acc = 0.7393333333333333, Val AUROC = 0.8063132762908936, Val F1 = 0.7379356622695923\n",
      "Epoch 3 : Training loss = 0.5433007959713768, Val loss = 0.68972520840168, Val Acc = 0.6643333333333333, Val AUROC = 0.8469883799552917, Val F1 = 0.5305361151695251\n",
      "Epoch 4 : Training loss = 0.5107885228714171, Val loss = 0.5320299098491669, Val Acc = 0.7576666666666667, Val AUROC = 0.8255977630615234, Val F1 = 0.7288325428962708\n",
      "Epoch 5 : Training loss = 0.5035613485274401, Val loss = 1.0014066227674485, Val Acc = 0.513, Val AUROC = 0.8582150936126709, Val F1 = 0.06165703386068344\n",
      "Epoch 6 : Training loss = 0.5011793017234165, Val loss = 0.5163153401613235, Val Acc = 0.758, Val AUROC = 0.8358248472213745, Val F1 = 0.7706885933876038\n",
      "Epoch 7 : Training loss = 0.4965715426712232, Val loss = 0.46649731367826464, Val Acc = 0.8146666666666667, Val AUROC = 0.8679315447807312, Val F1 = 0.8086717128753662\n",
      "Epoch 8 : Training loss = 0.492232694647306, Val loss = 0.6353651431798935, Val Acc = 0.666, Val AUROC = 0.865501344203949, Val F1 = 0.5308988690376282\n",
      "Epoch 9 : Training loss = 0.49060066617648623, Val loss = 0.5647220058441162, Val Acc = 0.729, Val AUROC = 0.8330057859420776, Val F1 = 0.6842718720436096\n",
      "Epoch 10 : Training loss = 0.4832531632320842, Val loss = 1.1068274572342633, Val Acc = 0.5086666666666667, Val AUROC = 0.8594475388526917, Val F1 = 0.0403645820915699\n",
      "training works\n",
      "Epoch 11 : Training loss = 0.4827140539424891, Val loss = 0.9479162484407425, Val Acc = 0.53, Val AUROC = 0.8544015884399414, Val F1 = 0.6785225868225098\n",
      "Epoch 12 : Training loss = 0.4852781365749258, Val loss = 1.345136413693428, Val Acc = 0.5016666666666667, Val AUROC = 0.85400390625, Val F1 = 0.006644518114626408\n",
      "Epoch 13 : Training loss = 0.480988687675226, Val loss = 0.49184704083204267, Val Acc = 0.78, Val AUROC = 0.8698937892913818, Val F1 = 0.7486671805381775\n",
      "Epoch 14 : Training loss = 0.47770402953986335, Val loss = 0.81687674087286, Val Acc = 0.5853333333333334, Val AUROC = 0.8598821759223938, Val F1 = 0.7015355229377747\n",
      "Epoch 15 : Training loss = 0.48174371152171336, Val loss = 0.9322642631530762, Val Acc = 0.529, Val AUROC = 0.8559058904647827, Val F1 = 0.12290503084659576\n",
      "Epoch 16 : Training loss = 0.47577406732874644, Val loss = 1.1425820398330688, Val Acc = 0.5, Val AUROC = 0.8063168525695801, Val F1 = 0.6666666865348816\n",
      "Epoch 17 : Training loss = 0.48004889121756356, Val loss = 1.2870666732788085, Val Acc = 0.5, Val AUROC = 0.7996850609779358, Val F1 = 0.0\n",
      "Epoch 18 : Training loss = 0.4774317309681738, Val loss = 1.4537121261358261, Val Acc = 0.5, Val AUROC = 0.8536697030067444, Val F1 = 0.0\n",
      "Epoch 19 : Training loss = 0.47248550520407, Val loss = 1.1066094564199447, Val Acc = 0.512, Val AUROC = 0.780501127243042, Val F1 = 0.671159029006958\n",
      "Epoch 20 : Training loss = 0.47293997919242814, Val loss = 2.6596859216690065, Val Acc = 0.5, Val AUROC = 0.802565336227417, Val F1 = 0.0\n",
      "training works\n",
      "Epoch 21 : Training loss = 0.47386460831099936, Val loss = 0.5514038293361664, Val Acc = 0.732, Val AUROC = 0.8786208629608154, Val F1 = 0.7766666412353516\n",
      "Epoch 22 : Training loss = 0.47214631299586407, Val loss = 1.2846527627706528, Val Acc = 0.504, Val AUROC = 0.7846675515174866, Val F1 = 0.6681534051895142\n",
      "Epoch 23 : Training loss = 0.4735371480921751, Val loss = 1.5373342280387878, Val Acc = 0.5006666666666667, Val AUROC = 0.8692394495010376, Val F1 = 0.0026631157379597425\n",
      "Epoch 24 : Training loss = 0.4788735216769175, Val loss = 1.622805517911911, Val Acc = 0.5, Val AUROC = 0.8449184894561768, Val F1 = 0.0\n",
      "Epoch 25 : Training loss = 0.4726294741711253, Val loss = 0.7457775578498841, Val Acc = 0.5806666666666667, Val AUROC = 0.8736889362335205, Val F1 = 0.30188679695129395\n",
      "Epoch 26 : Training loss = 0.472887191682058, Val loss = 0.5342893872261048, Val Acc = 0.7466666666666667, Val AUROC = 0.8723124861717224, Val F1 = 0.7837222814559937\n",
      "Epoch 27 : Training loss = 0.47165669822600936, Val loss = 1.2454807999134063, Val Acc = 0.5006666666666667, Val AUROC = 0.8530341982841492, Val F1 = 0.0026631157379597425\n",
      "Epoch 28 : Training loss = 0.4737107808020959, Val loss = 1.4825103645324706, Val Acc = 0.5, Val AUROC = 0.8514615893363953, Val F1 = 0.6666666865348816\n",
      "Epoch 29 : Training loss = 0.46862057457903866, Val loss = 0.43921306169033053, Val Acc = 0.824, Val AUROC = 0.8754465579986572, Val F1 = 0.8256275057792664\n",
      "Epoch 30 : Training loss = 0.4743668647050449, Val loss = 1.3052285697460175, Val Acc = 0.5, Val AUROC = 0.8468637466430664, Val F1 = 0.0\n",
      "training works\n",
      "Epoch 31 : Training loss = 0.4686683959805669, Val loss = 1.5854676548838615, Val Acc = 0.5016666666666667, Val AUROC = 0.8473709225654602, Val F1 = 0.6674082279205322\n",
      "Epoch 32 : Training loss = 0.47251614933353053, Val loss = 1.2828356512784957, Val Acc = 0.5016666666666667, Val AUROC = 0.836286723613739, Val F1 = 0.6674082279205322\n",
      "Epoch 33 : Training loss = 0.47189092233891966, Val loss = 0.8158684009313584, Val Acc = 0.529, Val AUROC = 0.8665517568588257, Val F1 = 0.6783519387245178\n",
      "Epoch 34 : Training loss = 0.46848874292061077, Val loss = 1.5719832260608673, Val Acc = 0.5013333333333333, Val AUROC = 0.7571224570274353, Val F1 = 0.6666666865348816\n",
      "Epoch 35 : Training loss = 0.4687411833136329, Val loss = 1.448099697113037, Val Acc = 0.5016666666666667, Val AUROC = 0.7380326390266418, Val F1 = 0.6671119928359985\n",
      "Epoch 36 : Training loss = 0.474023523206576, Val loss = 1.839193121433258, Val Acc = 0.5006666666666667, Val AUROC = 0.772154688835144, Val F1 = 0.6666666865348816\n",
      "Epoch 37 : Training loss = 0.4632727804608905, Val loss = 0.49758813256025314, Val Acc = 0.78, Val AUROC = 0.8615497946739197, Val F1 = 0.8025134801864624\n",
      "Epoch 38 : Training loss = 0.46353179705428993, Val loss = 1.2066125371456147, Val Acc = 0.5086666666666667, Val AUROC = 0.8684266805648804, Val F1 = 0.037859007716178894\n",
      "Epoch 39 : Training loss = 0.4681764750166029, Val loss = 1.6031175932884216, Val Acc = 0.5, Val AUROC = 0.8475146293640137, Val F1 = 0.0\n",
      "Epoch 40 : Training loss = 0.4658355031038005, Val loss = 1.72916592168808, Val Acc = 0.5, Val AUROC = 0.8540762662887573, Val F1 = 0.0\n",
      "training works\n",
      "Epoch 41 : Training loss = 0.46554333543614024, Val loss = 0.6615676611661911, Val Acc = 0.642, Val AUROC = 0.8440243601799011, Val F1 = 0.7260203957557678\n",
      "Epoch 42 : Training loss = 0.4662962961932381, Val loss = 2.3707932307720183, Val Acc = 0.5, Val AUROC = 0.8118732571601868, Val F1 = 0.0\n",
      "Epoch 43 : Training loss = 0.46553065892765433, Val loss = 0.9503683623969554, Val Acc = 0.5233333333333333, Val AUROC = 0.7512988448143005, Val F1 = 0.6763241291046143\n",
      "Epoch 44 : Training loss = 0.4623841374295332, Val loss = 1.831941682100296, Val Acc = 0.5, Val AUROC = 0.8221966624259949, Val F1 = 0.0\n",
      "Epoch 45 : Training loss = 0.46108000636764745, Val loss = 2.112683307647705, Val Acc = 0.5, Val AUROC = 0.7616732120513916, Val F1 = 0.0\n",
      "Epoch 46 : Training loss = 0.4578116705454407, Val loss = 1.011914154291153, Val Acc = 0.45566666666666666, Val AUROC = 0.16434843838214874, Val F1 = 0.6236460208892822\n",
      "Epoch 47 : Training loss = 0.45841103060539706, Val loss = 1.3305337505340575, Val Acc = 0.501, Val AUROC = 0.7863810658454895, Val F1 = 0.6668150424957275\n",
      "Epoch 48 : Training loss = 0.4583295717924808, Val loss = 1.0215859034061432, Val Acc = 0.5103333333333333, Val AUROC = 0.8755257725715637, Val F1 = 0.04299674183130264\n",
      "Epoch 49 : Training loss = 0.4597681345930443, Val loss = 0.49796035534143446, Val Acc = 0.7736666666666666, Val AUROC = 0.8629331588745117, Val F1 = 0.7976155281066895\n",
      "Epoch 50 : Training loss = 0.458708317837045, Val loss = 0.6664342043399811, Val Acc = 0.6073333333333333, Val AUROC = 0.871749997138977, Val F1 = 0.3773784339427948\n",
      "training works\n",
      "Epoch 51 : Training loss = 0.4586372592812843, Val loss = 1.755174349784851, Val Acc = 0.5, Val AUROC = 0.8473368883132935, Val F1 = 0.0\n",
      "Epoch 52 : Training loss = 0.4598364247122481, Val loss = 1.3053707277178765, Val Acc = 0.5243333333333333, Val AUROC = 0.8740999698638916, Val F1 = 0.10644959658384323\n",
      "Epoch 53 : Training loss = 0.4602307659009633, Val loss = 0.8933365225791932, Val Acc = 0.5486666666666666, Val AUROC = 0.8839461803436279, Val F1 = 0.190191388130188\n",
      "Epoch 54 : Training loss = 0.4550420018655169, Val loss = 1.7598733336925507, Val Acc = 0.5, Val AUROC = 0.8681537508964539, Val F1 = 0.0\n",
      "Epoch 55 : Training loss = 0.45666680739747223, Val loss = 1.1569536368250848, Val Acc = 0.5043333333333333, Val AUROC = 0.8746587038040161, Val F1 = 0.01977587305009365\n",
      "Epoch 56 : Training loss = 0.4512621846557583, Val loss = 1.7398191556930542, Val Acc = 0.5, Val AUROC = 0.8415762186050415, Val F1 = 0.0\n",
      "Epoch 57 : Training loss = 0.45817417954673295, Val loss = 1.4268854643106461, Val Acc = 0.5023333333333333, Val AUROC = 0.8751497268676758, Val F1 = 0.6675573587417603\n",
      "Epoch 58 : Training loss = 0.4518369476230306, Val loss = 1.7405994111299514, Val Acc = 0.5, Val AUROC = 0.8050462007522583, Val F1 = 0.0\n",
      "Epoch 59 : Training loss = 0.4583854323130136, Val loss = 1.944798975944519, Val Acc = 0.5, Val AUROC = 0.8288488388061523, Val F1 = 0.0\n",
      "Epoch 60 : Training loss = 0.4629610677943234, Val loss = 1.3760654950141906, Val Acc = 0.5, Val AUROC = 0.8251342177391052, Val F1 = 0.0\n",
      "training works\n",
      "Epoch 61 : Training loss = 0.4569261351020906, Val loss = 0.424786282658577, Val Acc = 0.8376666666666667, Val AUROC = 0.8815226554870605, Val F1 = 0.8320110440254211\n",
      "Epoch 62 : Training loss = 0.4562491109561185, Val loss = 0.4425481294989586, Val Acc = 0.8196666666666667, Val AUROC = 0.8781315088272095, Val F1 = 0.8290679454803467\n",
      "Epoch 63 : Training loss = 0.45490787306604163, Val loss = 1.2370806279182434, Val Acc = 0.5036666666666667, Val AUROC = 0.8242167234420776, Val F1 = 0.6675597429275513\n",
      "Epoch 64 : Training loss = 0.44987862289666924, Val loss = 1.1734493292570114, Val Acc = 0.5143333333333333, Val AUROC = 0.7835021615028381, Val F1 = 0.6723634004592896\n",
      "Epoch 65 : Training loss = 0.4593499643024258, Val loss = 1.0633798272013664, Val Acc = 0.517, Val AUROC = 0.8079766035079956, Val F1 = 0.6732807159423828\n",
      "Epoch 66 : Training loss = 0.4570928558949844, Val loss = 0.6644653218388558, Val Acc = 0.6213333333333333, Val AUROC = 0.7929515838623047, Val F1 = 0.7082691192626953\n",
      "Epoch 67 : Training loss = 0.45502603655455964, Val loss = 0.6979657838344574, Val Acc = 0.656, Val AUROC = 0.8840026259422302, Val F1 = 0.5052732229232788\n",
      "Epoch 68 : Training loss = 0.4512924175284924, Val loss = 1.266403634786606, Val Acc = 0.5056666666666667, Val AUROC = 0.840666651725769, Val F1 = 0.6686033606529236\n",
      "Epoch 69 : Training loss = 0.4583026510507711, Val loss = 2.7941671159267427, Val Acc = 0.5, Val AUROC = 0.6059893369674683, Val F1 = 0.0\n",
      "Epoch 70 : Training loss = 0.45503119796182523, Val loss = 1.6279120182991027, Val Acc = 0.5006666666666667, Val AUROC = 0.7837169170379639, Val F1 = 0.666814923286438\n",
      "training works\n",
      "Epoch 71 : Training loss = 0.4527164302453488, Val loss = 2.223807846546173, Val Acc = 0.5, Val AUROC = 0.8441177606582642, Val F1 = 0.0\n",
      "Epoch 72 : Training loss = 0.45584451460184555, Val loss = 0.41858669072389604, Val Acc = 0.841, Val AUROC = 0.8872008323669434, Val F1 = 0.836026132106781\n",
      "Epoch 73 : Training loss = 0.45890133605109595, Val loss = 1.3026239321231843, Val Acc = 0.5, Val AUROC = 0.8335427045822144, Val F1 = 0.0\n",
      "Epoch 74 : Training loss = 0.45130927862013726, Val loss = 1.5168187849521637, Val Acc = 0.5006666666666667, Val AUROC = 0.8690946102142334, Val F1 = 0.0026631157379597425\n",
      "Epoch 75 : Training loss = 0.45058468413230385, Val loss = 0.5600904179811478, Val Acc = 0.705, Val AUROC = 0.8822692632675171, Val F1 = 0.6120122671127319\n",
      "Epoch 76 : Training loss = 0.4514587180371354, Val loss = 0.5187121928930283, Val Acc = 0.7516666666666667, Val AUROC = 0.8775101900100708, Val F1 = 0.7872036695480347\n",
      "Epoch 77 : Training loss = 0.4544767405655819, Val loss = 0.8848420013189315, Val Acc = 0.544, Val AUROC = 0.8632940053939819, Val F1 = 0.6846473217010498\n",
      "Epoch 78 : Training loss = 0.4500317488124254, Val loss = 0.42822454392910003, Val Acc = 0.8373333333333334, Val AUROC = 0.8797484636306763, Val F1 = 0.831375241279602\n",
      "Epoch 79 : Training loss = 0.47679377513863436, Val loss = 1.3961666216850281, Val Acc = 0.5, Val AUROC = 0.8411402106285095, Val F1 = 0.0\n",
      "Epoch 80 : Training loss = 0.4540729218169779, Val loss = 1.8385853127241134, Val Acc = 0.5, Val AUROC = 0.7676123976707458, Val F1 = 0.0\n",
      "training works\n",
      "Epoch 81 : Training loss = 0.4514908957476277, Val loss = 2.188034938812256, Val Acc = 0.5, Val AUROC = 0.7937595248222351, Val F1 = 0.0\n",
      "Epoch 82 : Training loss = 0.4547083327656846, Val loss = 0.578291040122509, Val Acc = 0.692, Val AUROC = 0.8552901744842529, Val F1 = 0.7504051923751831\n",
      "Epoch 83 : Training loss = 0.4538764187103678, Val loss = 0.7030323199033737, Val Acc = 0.609, Val AUROC = 0.868083119392395, Val F1 = 0.7120059132575989\n",
      "Epoch 84 : Training loss = 0.44686622344334104, Val loss = 0.4130979058742523, Val Acc = 0.8476666666666667, Val AUROC = 0.8906688690185547, Val F1 = 0.8463865518569946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x14553255dc60>\n",
      "Traceback (most recent call last):\n",
      "  File \"/ext3/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1478, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/ext3/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1400, in _shutdown_workers\n",
      "    def _shutdown_workers(self):\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m start_training( model, train_dataloader, val_dataloader, device)\n",
      "Cell \u001b[0;32mIn[33], line 28\u001b[0m, in \u001b[0;36mstart_training\u001b[0;34m(model, train_dataloader, val_dataloader, device)\u001b[0m\n\u001b[1;32m     26\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     27\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 28\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;66;03m#print('iteration done')\u001b[39;00m\n\u001b[1;32m     30\u001b[0m     \n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m#print(f'epoch {epoch}, train done')\u001b[39;00m\n\u001b[1;32m     33\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m train_loss\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_dataloader)\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/site-packages/torch/optim/optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    370\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    371\u001b[0m             )\n\u001b[0;32m--> 373\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    376\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/site-packages/torch/optim/adam.py:163\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    152\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    155\u001b[0m         group,\n\u001b[1;32m    156\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    160\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    161\u001b[0m         state_steps)\n\u001b[0;32m--> 163\u001b[0m     adam(\n\u001b[1;32m    164\u001b[0m         params_with_grad,\n\u001b[1;32m    165\u001b[0m         grads,\n\u001b[1;32m    166\u001b[0m         exp_avgs,\n\u001b[1;32m    167\u001b[0m         exp_avg_sqs,\n\u001b[1;32m    168\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    169\u001b[0m         state_steps,\n\u001b[1;32m    170\u001b[0m         amsgrad\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mamsgrad\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    171\u001b[0m         beta1\u001b[38;5;241m=\u001b[39mbeta1,\n\u001b[1;32m    172\u001b[0m         beta2\u001b[38;5;241m=\u001b[39mbeta2,\n\u001b[1;32m    173\u001b[0m         lr\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    174\u001b[0m         weight_decay\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    175\u001b[0m         eps\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meps\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    176\u001b[0m         maximize\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    177\u001b[0m         foreach\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mforeach\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    178\u001b[0m         capturable\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcapturable\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    179\u001b[0m         differentiable\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    180\u001b[0m         fused\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfused\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    181\u001b[0m         grad_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad_scale\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    182\u001b[0m         found_inf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    183\u001b[0m     )\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/site-packages/torch/optim/adam.py:311\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    309\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 311\u001b[0m func(params,\n\u001b[1;32m    312\u001b[0m      grads,\n\u001b[1;32m    313\u001b[0m      exp_avgs,\n\u001b[1;32m    314\u001b[0m      exp_avg_sqs,\n\u001b[1;32m    315\u001b[0m      max_exp_avg_sqs,\n\u001b[1;32m    316\u001b[0m      state_steps,\n\u001b[1;32m    317\u001b[0m      amsgrad\u001b[38;5;241m=\u001b[39mamsgrad,\n\u001b[1;32m    318\u001b[0m      beta1\u001b[38;5;241m=\u001b[39mbeta1,\n\u001b[1;32m    319\u001b[0m      beta2\u001b[38;5;241m=\u001b[39mbeta2,\n\u001b[1;32m    320\u001b[0m      lr\u001b[38;5;241m=\u001b[39mlr,\n\u001b[1;32m    321\u001b[0m      weight_decay\u001b[38;5;241m=\u001b[39mweight_decay,\n\u001b[1;32m    322\u001b[0m      eps\u001b[38;5;241m=\u001b[39meps,\n\u001b[1;32m    323\u001b[0m      maximize\u001b[38;5;241m=\u001b[39mmaximize,\n\u001b[1;32m    324\u001b[0m      capturable\u001b[38;5;241m=\u001b[39mcapturable,\n\u001b[1;32m    325\u001b[0m      differentiable\u001b[38;5;241m=\u001b[39mdifferentiable,\n\u001b[1;32m    326\u001b[0m      grad_scale\u001b[38;5;241m=\u001b[39mgrad_scale,\n\u001b[1;32m    327\u001b[0m      found_inf\u001b[38;5;241m=\u001b[39mfound_inf)\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/site-packages/torch/optim/adam.py:496\u001b[0m, in \u001b[0;36m_multi_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    493\u001b[0m device_params \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mview_as_real(x) \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_complex(x) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m device_params]\n\u001b[1;32m    495\u001b[0m \u001b[38;5;66;03m# update steps\u001b[39;00m\n\u001b[0;32m--> 496\u001b[0m torch\u001b[38;5;241m.\u001b[39m_foreach_add_(device_state_steps, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weight_decay \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    499\u001b[0m     \u001b[38;5;66;03m# Re-use the intermediate memory (device_grads) already allocated for maximize\u001b[39;00m\n\u001b[1;32m    500\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m maximize:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_training( model, train_dataloader, val_dataloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchgeo",
   "language": "python",
   "name": "pytorch-example"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
